{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Automated Data Collection from OpenPrescribing API**\n",
        "\n",
        "This script implements an automated data collection system to retrieve diabetes prescription data for West Yorkshire from the OpenPrescribing.net API. The script systematically downloads prescription records for seven categories of diabetes-related medications (insulin, sulfonylureas, biguanides, and other antidiabetic drugs) across all eleven Sub-Integrated Care Board (subICB) areas within West Yorkshire for the period 2020-2025.\n",
        "\n",
        "Key features include:\n",
        "\n",
        "(1) memory-optimized processing using chunked data handling to manage large datasets;\n",
        "\n",
        "(2) systematic API querying with rate-limiting compliance;\n",
        "\n",
        "(3) data filtering to retain only primary care prescribing data (setting=4);\n",
        "\n",
        "(4) automated deduplication and data quality validation;\n",
        "\n",
        "(5) transformation from long-format to wide-format data structure for subsequent analysis.\n",
        "\n",
        "The output is a comprehensive dataset where each row represents a GP practice's monthly prescribing activity across all diabetes medication categories.\n",
        "\n",
        "Could take up to 20 minutes."
      ],
      "metadata": {
        "id": "0L3LrhLv9jHY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YiJGrUX9iYx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "from datetime import datetime\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# Memory optimization settings\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "\n",
        "# Drug categories and BNF code mapping\n",
        "drug_groups = [\n",
        "    (\"6.1.1.1\", \"Short-acting insulins\", [\"0601011\"]),\n",
        "    (\"6.1.1.2\", \"Intermediate and long-acting insulins\", [\"0601012\"]),\n",
        "    (\"6.1.2.1\", \"Sulfonylureas\", [\"0601021\"]),\n",
        "    (\"6.1.2.2\", \"Biguanides\", [\"0601022\"]),\n",
        "    (\"6.1.2.3\", \"Other antidiabetic drugs\", [\"0601023\"]),\n",
        "    (\"6.1.4\", \"Treatment of hypoglycaemia\", [\"0601040\"]),\n",
        "    (\"6.1.6\", \"Diabetic diagnostic and monitoring agents\", [\"0601060\"])\n",
        "]\n",
        "\n",
        "group_order = [g[1] for g in drug_groups]\n",
        "\n",
        "# West Yorkshire subICB codes (derived from manual analysis of annual patient distribution data)\n",
        "all_west_yorkshire_codes = [\n",
        "    '02N', '02R', '02T', '02W', '03A', '03E', '03J', '03R', '15F', '36J', 'X2C4Y'\n",
        "]\n",
        "\n",
        "# Output file configuration\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "temp_file = f\"temp_raw_data_{timestamp}.csv\"\n",
        "output_file = f\"WestYorkshire_GPDrugData_by_Practice.csv\"\n",
        "\n",
        "print(\"Starting West Yorkshire data download (memory optimized version)...\")\n",
        "print(f\"Temporary data will be saved to: {temp_file}\")\n",
        "\n",
        "# Data collection with direct file writing to avoid memory accumulation\n",
        "base_url = \"https://openprescribing.net/api/1.0/spending_by_org/\"\n",
        "total_api_calls = 0\n",
        "successful_calls = 0\n",
        "is_first_write = True\n",
        "\n",
        "for ch, group_name, drugcodes in drug_groups:\n",
        "    for drug_code in drugcodes:\n",
        "        print(f\"Processing: {group_name} ({drug_code})\")\n",
        "\n",
        "        for subicb_code in all_west_yorkshire_codes:\n",
        "            total_api_calls += 1\n",
        "            full_url = f\"{base_url}?org_type=practice&code={drug_code}&org={subicb_code}&format=csv\"\n",
        "\n",
        "            try:\n",
        "                response = requests.get(full_url, timeout=180)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                if len(response.content) > 100:\n",
        "                    df = pd.read_csv(io.StringIO(response.text))\n",
        "                    if len(df) > 0:\n",
        "                        # Filter for setting=4 data only\n",
        "                        df_filtered = df[df['setting'] == 4].copy()\n",
        "                        if len(df_filtered) > 0:\n",
        "                            # Add identifier columns\n",
        "                            df_filtered['drug_code'] = drug_code\n",
        "                            df_filtered['drug_group'] = group_name\n",
        "\n",
        "                            # Optimize data types for memory efficiency\n",
        "                            df_filtered['date'] = pd.to_datetime(df_filtered['date'])\n",
        "                            df_filtered['items'] = pd.to_numeric(df_filtered['items'], downcast='integer')\n",
        "                            df_filtered['quantity'] = pd.to_numeric(df_filtered['quantity'], downcast='float')\n",
        "                            df_filtered['actual_cost'] = pd.to_numeric(df_filtered['actual_cost'], downcast='float')\n",
        "\n",
        "                            # Keep only required columns\n",
        "                            columns_needed = ['ccg', 'row_id', 'row_name', 'date',\n",
        "                                            'items', 'quantity', 'actual_cost',\n",
        "                                            'drug_code', 'drug_group']\n",
        "                            df_filtered = df_filtered[columns_needed]\n",
        "\n",
        "                            # Write directly to temporary file instead of storing in memory\n",
        "                            if is_first_write:\n",
        "                                df_filtered.to_csv(temp_file, mode='w', index=False)\n",
        "                                is_first_write = False\n",
        "                            else:\n",
        "                                df_filtered.to_csv(temp_file, mode='a', header=False, index=False)\n",
        "\n",
        "                            successful_calls += 1\n",
        "                            print(f\"Success {subicb_code}: {len(df_filtered)} rows (written to file)\")\n",
        "\n",
        "                            del df_filtered\n",
        "                        else:\n",
        "                            print(f\"No setting=4 data for {subicb_code}\")\n",
        "                    else:\n",
        "                        print(f\"Empty data for {subicb_code}\")\n",
        "\n",
        "                    del df\n",
        "                else:\n",
        "                    print(f\"No data for {subicb_code}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed {subicb_code}: {str(e)[:50]}\")\n",
        "                if \"429\" in str(e):\n",
        "                    print(\"Rate limit encountered, waiting 10 seconds...\")\n",
        "                    time.sleep(10)\n",
        "                    continue\n",
        "\n",
        "            gc.collect()\n",
        "            time.sleep(0.5)\n",
        "\n",
        "print(f\"Data collection completed:\")\n",
        "print(f\"   Total API calls: {total_api_calls}\")\n",
        "print(f\"   Successful calls: {successful_calls}\")\n",
        "\n",
        "if successful_calls == 0:\n",
        "    print(\"No data collected!\")\n",
        "    exit()\n",
        "\n",
        "# Process temporary file in chunks to avoid memory overflow\n",
        "print(\"Starting chunked data processing...\")\n",
        "\n",
        "chunk_size = 10000\n",
        "processed_chunks = []\n",
        "chunk_files = []\n",
        "\n",
        "try:\n",
        "    chunk_counter = 0\n",
        "    for chunk in pd.read_csv(temp_file, chunksize=chunk_size):\n",
        "        chunk_counter += 1\n",
        "        print(f\"Processing chunk {chunk_counter}: {len(chunk)} rows\")\n",
        "\n",
        "        # Optimize data types\n",
        "        chunk['date'] = pd.to_datetime(chunk['date'])\n",
        "        chunk['items'] = pd.to_numeric(chunk['items'], downcast='integer')\n",
        "        chunk['quantity'] = pd.to_numeric(chunk['quantity'], downcast='float')\n",
        "        chunk['actual_cost'] = pd.to_numeric(chunk['actual_cost'], downcast='float')\n",
        "\n",
        "        # Deduplicate within chunk\n",
        "        dedup_columns = ['ccg', 'row_id', 'row_name', 'date', 'drug_code', 'drug_group']\n",
        "        chunk_deduped = chunk.groupby(dedup_columns, as_index=False).agg({\n",
        "            'items': 'sum',\n",
        "            'quantity': 'sum',\n",
        "            'actual_cost': 'sum'\n",
        "        })\n",
        "\n",
        "        # Save processed chunk to temporary file\n",
        "        chunk_file = f\"chunk_{chunk_counter}_{timestamp}.csv\"\n",
        "        chunk_deduped.to_csv(chunk_file, index=False)\n",
        "        chunk_files.append(chunk_file)\n",
        "\n",
        "        print(f\"Chunk deduplication completed: {len(chunk)} rows â†’ {len(chunk_deduped)} rows\")\n",
        "\n",
        "        del chunk, chunk_deduped\n",
        "        gc.collect()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Chunked processing failed: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Merge all processed chunks\n",
        "print(\"Merging all processed data chunks...\")\n",
        "final_deduped_file = f\"final_deduped_{timestamp}.csv\"\n",
        "is_first_merge = True\n",
        "\n",
        "for chunk_file in chunk_files:\n",
        "    chunk_data = pd.read_csv(chunk_file)\n",
        "\n",
        "    if is_first_merge:\n",
        "        chunk_data.to_csv(final_deduped_file, mode='w', index=False)\n",
        "        is_first_merge = False\n",
        "    else:\n",
        "        chunk_data.to_csv(final_deduped_file, mode='a', header=False, index=False)\n",
        "\n",
        "    del chunk_data\n",
        "    gc.collect()\n",
        "    os.remove(chunk_file)\n",
        "\n",
        "os.remove(temp_file)\n",
        "print(\"Temporary file cleanup completed\")\n",
        "\n",
        "# Final deduplication (handle cross-chunk duplicates)\n",
        "print(\"Executing final deduplication...\")\n",
        "final_data_list = []\n",
        "dedup_columns = ['ccg', 'row_id', 'row_name', 'date', 'drug_code', 'drug_group']\n",
        "\n",
        "for chunk in pd.read_csv(final_deduped_file, chunksize=chunk_size):\n",
        "    final_data_list.append(chunk)\n",
        "\n",
        "# Merge and perform final deduplication\n",
        "df_final = pd.concat(final_data_list, ignore_index=True)\n",
        "del final_data_list\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Before final deduplication: {len(df_final)} rows\")\n",
        "df_final = df_final.groupby(dedup_columns, as_index=False).agg({\n",
        "    'items': 'sum',\n",
        "    'quantity': 'sum',\n",
        "    'actual_cost': 'sum'\n",
        "})\n",
        "print(f\"After final deduplication: {len(df_final)} rows\")\n",
        "\n",
        "os.remove(final_deduped_file)\n",
        "\n",
        "# Convert to wide table format (batch processing)\n",
        "print(\"Converting to wide table format...\")\n",
        "\n",
        "# Group by practice and date, process in batches\n",
        "practice_dates = df_final[['ccg', 'row_id', 'row_name', 'date']].drop_duplicates()\n",
        "print(f\"Unique practice-date combinations: {len(practice_dates)}\")\n",
        "\n",
        "# Create final wide table structure\n",
        "expected_cols = (['ccg', 'row_id', 'row_name', 'date'] +\n",
        "                [f\"{group}_{suffix}\"\n",
        "                 for group in group_order\n",
        "                 for suffix in ['items', 'quantity', 'actual_cost']])\n",
        "\n",
        "# Initialize result DataFrame\n",
        "result_chunks = []\n",
        "batch_size = 1000\n",
        "\n",
        "for i in range(0, len(practice_dates), batch_size):\n",
        "    batch_practices = practice_dates.iloc[i:i+batch_size]\n",
        "    print(f\"Processing batch {i//batch_size + 1}/{(len(practice_dates)-1)//batch_size + 1}\")\n",
        "\n",
        "    # Create empty wide table for current batch\n",
        "    batch_result = batch_practices.copy()\n",
        "\n",
        "    # Add columns for each drug group\n",
        "    for group in group_order:\n",
        "        for metric in ['items', 'quantity', 'actual_cost']:\n",
        "            if metric == 'items':\n",
        "                batch_result[f\"{group}_{metric}\"] = 0\n",
        "            else:\n",
        "                batch_result[f\"{group}_{metric}\"] = 0.0\n",
        "\n",
        "    # Fill actual data\n",
        "    for _, practice_row in batch_practices.iterrows():\n",
        "        # Find all drug data for this practice-date\n",
        "        mask = ((df_final['ccg'] == practice_row['ccg']) &\n",
        "                (df_final['row_id'] == practice_row['row_id']) &\n",
        "                (df_final['date'] == practice_row['date']))\n",
        "\n",
        "        practice_drugs = df_final[mask]\n",
        "\n",
        "        # Fill data for each drug group\n",
        "        for _, drug_row in practice_drugs.iterrows():\n",
        "            group = drug_row['drug_group']\n",
        "            for metric in ['items', 'quantity', 'actual_cost']:\n",
        "                col_name = f\"{group}_{metric}\"\n",
        "                if col_name in batch_result.columns:\n",
        "                    batch_mask = ((batch_result['ccg'] == practice_row['ccg']) &\n",
        "                                 (batch_result['row_id'] == practice_row['row_id']) &\n",
        "                                 (batch_result['date'] == practice_row['date']))\n",
        "                    batch_result.loc[batch_mask, col_name] = drug_row[metric]\n",
        "\n",
        "    result_chunks.append(batch_result)\n",
        "\n",
        "    del batch_result, batch_practices\n",
        "    gc.collect()\n",
        "\n",
        "# Merge all batch results\n",
        "print(\"Merging all batch results...\")\n",
        "final_wide_df = pd.concat(result_chunks, ignore_index=True)\n",
        "del result_chunks\n",
        "gc.collect()\n",
        "\n",
        "# Add missing quantity columns\n",
        "for group in group_order:\n",
        "    for suffix in ['quantity']:\n",
        "        col_name = f\"{group}_{suffix}\"\n",
        "        if col_name not in final_wide_df.columns:\n",
        "            final_wide_df[col_name] = 0.0\n",
        "\n",
        "# Ensure correct column order\n",
        "final_wide_df = final_wide_df[expected_cols]\n",
        "\n",
        "# Final data quality check\n",
        "print(\"Final data quality check:\")\n",
        "print(f\"   Final rows: {len(final_wide_df):,}\")\n",
        "print(f\"   Final columns: {len(final_wide_df.columns)}\")\n",
        "print(f\"   Date range: {final_wide_df['date'].min()} to {final_wide_df['date'].max()}\")\n",
        "print(f\"   Unique SubICBs: {final_wide_df['ccg'].nunique()}\")\n",
        "print(f\"   Unique GP practices: {final_wide_df['row_id'].nunique()}\")\n",
        "\n",
        "# Save final result\n",
        "print(f\"Saving to file: {output_file}\")\n",
        "final_wide_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(\"Process completed!\")\n",
        "print(f\"File saved: {output_file}\")\n",
        "print(f\"Final data: {len(final_wide_df):,} rows Ã— {len(final_wide_df.columns)} columns\")\n",
        "\n",
        "# Generate data statistics report\n",
        "print(\"Data statistics report:\")\n",
        "for group in group_order:\n",
        "    items_col = f\"{group}_items\"\n",
        "    if items_col in final_wide_df.columns:\n",
        "        total_items = final_wide_df[items_col].sum()\n",
        "        practices_prescribing = (final_wide_df[items_col] > 0).sum()\n",
        "        print(f\"   {group}:\")\n",
        "        print(f\"     Total prescriptions: {total_items:,}\")\n",
        "        print(f\"     Practice-month entries with prescriptions: {practices_prescribing:,}\")\n",
        "\n",
        "# Display memory usage\n",
        "try:\n",
        "    import psutil\n",
        "    process = psutil.Process()\n",
        "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "    print(f\"Current memory usage: {memory_mb:.1f} MB\")\n",
        "except ImportError:\n",
        "    print(\"Install psutil to view memory usage: !pip install psutil\")\n",
        "\n",
        "# Attempt file download if in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_file)\n",
        "    print(\"File automatically downloaded\")\n",
        "except ImportError:\n",
        "    print(\"Manual file save required\")\n",
        "\n",
        "# Final cleanup\n",
        "del final_wide_df\n",
        "gc.collect()\n",
        "print(\"Memory cleanup completed\")"
      ]
    }
  ]
}